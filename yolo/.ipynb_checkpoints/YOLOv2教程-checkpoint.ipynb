{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv2是在YOLOv1的基础上作了改进而提出的，其在mAP有着显著的提升，同时保持检测的速度，保持着作为one-stage方法的优势，下图展示了YOLOv2对其他算法在VOC 2007数据集上的效果对比：\n",
    "<center> ![performance](./pic/yolov2.png) </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系列检测方法。YOLOv1在物体定位方面不够准确，并且召回率较低。YOLOv2提出几种改进策略来提升YOLOv1算法的定位准确度和召回率，从而提高mAP, YOLOv2在改进中遵循一个原则： 保持检测的速度。\n",
    "## Batch Normalizaion\n",
    "batch normalization可以提升模型的收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加batch normalization层，并且不再使用dropout。 使用batch normalization后，YOLOv2的mAP提升了2.4%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Resolution Classifier\n",
    "目前大部分的检测模型都会先在ImageNet分类数据集上预训练模型的主体部分，ImageNet分类模型基本采用大小为224x224的图片作为输入，分辨率相对较低，不利于检测模型，所以YOLOv1在采用224x224分类模型预训练后，将分辨率增加至448x448,并使用这个高分辨率在检测数据集上微调，但是直接切换分辨率，检测模型可能难以快速适应高分率，所以YOLOv2增加在ImageNet数据集上使用448x448输入来微调分类网络这一中间过程，这可以使得模型在检测数据集上微调之前已经适应高分辨率输入，使用高分辨率分类器后，YOLOv2的mAP提升约4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional With Anchors Boxes\n",
    "在YOLOv1中，输入图片最终被划分7x7网格，每个网格预测两个边界框。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽和高都是相对于整张图，而由于各个图片中存在不同尺度和长宽比(scales and ratios)的物体，YOLOv1在训练过程中学习适应不同物体的形状，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络中使用的先验框(anchor boxes)策略，采用先验框使得模型更容易学习。所以YOLOv2移除YOLOv1中的全连接层而采用anchor boxes来预测边界框，为了使检测所用的特征图分辨率高，移除其中的一个pool层。在检测模型中，YOLOv2不是采用448x448图片作为输入，而是采用416x416大小，因为YOLOv2模型下采样的总步长是32，对于416x416大小的图片，最终得到的特征图大小为13x13，维度是奇数，这样特征图恰好只有一个中心位置，对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易点。所以在YOLOv2设计中要保证最终的特征图有奇数个位置，对于YOLOv1，每个网格都预测两个bounding boxes, 每个boxes包含5个值(x,y,w,h,c),但是每个网格共用一套分类概率值(class predictions). YOLOv2使用anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值。使用anchor boxes之后，YOLOv2的mAP有稍微下降，YOLOv1只能预测98个边界框，而YOLOv2使用anchor boxes之后可以预测上千个边界框，所以使用anchor boxes之后，YOLOv2的召回率大大提升，由原来的81%升至88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Clusters\n",
    "在Faster R-CNN和SSD中，先验框的维度(长和宽)都是手动设定的，带有一定的主观性，如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。 因此YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析，因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用bounding box与聚类中心box之间的IOU作为距离度量\n",
    "\n",
    "$$ d(box,centroid) = 1 - IOU(box, centroid) $$\n",
    "<center> ![cluster](./pic/cluster.png)</center>\n",
    "\n",
    "论文作者对比了采用聚类分析得到的先验框与手动设置的先验框在平均IOU上的差异，发现前者的平均IOU值更高，因此模型更容易训练学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darknet-19\n",
    "YOLOv2采用一个新的网络模型充当特征提取器，称为Darknet-19， 包含19个卷积层和5个最大池化层。Darknet-19与VGG16模型设计原则是一致的，主要是采用3x3卷积。Darknet19最后使用global avgpooling做预测，并且在3x3卷积之间使用1x1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19的top-1准确率为72.9%，top-5准确率为91.2%,但是模型参数相对小一些，使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。\n",
    "<center> ![darknet](./pic/darknet19.png) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct location prediction\n",
    "YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置$(x,y)$, 需要根据预测的坐标偏移值$(t_{x},t_{y})$,先验框的尺度$(w_{a},h_{a})$以及中心坐标$(x_{a},y_{a})$来计算\n",
    "\n",
    "$$ x = (t_{x}\\times w_{a}) - x_{a} $$\n",
    "$$ y = (t_{y}\\times h_{a}) - y_{a} $$\n",
    "YOLOv2沿用YOLOv1的方法, 就是预测边界框中心点相对于对应网格左上角位置的相对偏移值，为了将边界框中心点约束在当前网格中，使用Sigmoid函数处理偏移值，这样预测的偏移值在（0,1）范围内（每个网络的尺度看成1），总而言之，根据边界框预测的4个offsets $t_{x},t_{y},t_{w},t_{h}$,可以按照如下公式计算出边界框实际位置和大小：\n",
    "\n",
    "$$ b_{x} = \\sigma(t_{x}) + c_{x} $$\n",
    "$$ b_{y} = \\sigma(t_{y}) + c_{y} $$\n",
    "$$ b_{w} = p_{w}e^{t_{w}} $$\n",
    "$$ b_{h} = p_{h}e^{t_{h}} $$\n",
    "其中$(c_{x},c_{y})$为当前网格的左上角坐标，在计算时每个网格的尺度为1,所以当前网格的左上角坐标为$(1,1)$,由于sigmoid函数的处理，边界框的中心位置会约束在当前网格内部，防止偏移过多, 而$p_{w}$和$p_{h}$是先验框的宽度和长度，它们的值是相对于特诊图大小的，在特征图中每个网格的长和宽均是1，这里记特征图的大小是$(W,H)$(论文中是$(13,13)$),这样我们可以将边界框相对于整张图片的位置和大小计算出来\n",
    "$$ b_{x} = (\\sigma(t_{x} + c_{x})/W $$\n",
    "$$ b_{y} = (\\sigma(t_{y} + c_{y})/H $$\n",
    "$$ b_{w} = p_{w}e^{t_{w}}/W $$\n",
    "$$ b_{h} = p_{h}e^{t_{h}}/H $$\n",
    "上面的4个值分别乘以图片的宽度和长度就可以得到边界框的最终位置和大小，这就是YOLOv2边界框的整个解码过程，约束边界框的位置预测值使得模型更容易稳定训练。\n",
    "<center> ![location](./pic/location.png) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Grained Features\n",
    "YOLOv2的输入图片大小为416x416, 经过5次池化操作之后得到13x13大小的特征图，并以此特征图采用卷积做预测。13x13大小的特征图对检测大物体是足够的，但是对于小物体还需要更精细的特征图（Fine-Grained Features）, YOLOv2提出一种passthough层利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26x26大小的特征图。对于Darknet-19模型来说就是大小为26x26x512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上，前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2x2的局部区域，然后将其转化为channel维度，对于26x26x512的特征图，经passthrough层处理之后就变成13x13x2048的新特征。这样就可以与后面的13x13x1024特征图连接在一起形成13x13x3072大小的特征图，然后在特征图基础上卷积做预测。\n",
    "<center> ![reorg](./pic/reorg.png) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scale Training\n",
    "由于YOLOv2模型只有卷积层和池化层，所以YOLOv2的输入可以不限于416x416大小的图片，为了增强模型的鲁棒性，YOLOv2采用多尺度训练策略，具体来说就是在训练过程中每隔一定的iterations之后改变模型的输入图片大小，由于YOLOv2的下采样总步长是32，输入图片大小选择一系列为32倍数的值：${320,352,...,608}$,输入图片最小为320x320,而输入图片最大为608x608，在训练的过程中每隔10个iterations随机选择一种输入图片大小，然后只需修改对最后检测层的处理就可以重新训练。\n",
    "<center> ![multi-scale](./multi-scale.png)</center>\n",
    "采用multi-scale training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果，在测试时，YOLOv2可以采用不同大小的图片作为输入。在VOC 2007数据集上的效果如图所示，可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨率输入时，mAP值更高，但是速度略有下降，对于544x544， mAP高达78.6%\n",
    "<center> ![result](./pic/result.png)</center>\n",
    "\n",
    "总的来说，虽然YOLOv2做了许多改进，但是大部分都是借鉴其他论文的一些技巧，如Faster-RCNN的anchor boxes, YOLOv2采用anchor boxes和卷积做预测，这基本上和SSD模型非常类似，而且SSD也是借鉴Faster R-CNN的RPN网络，从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，只不过RPN不做类别的预测，只是简单的区分物体与背景。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是做出粗糙的检测，所以另外增加一个stage，即采用R-CNN网络来进一步提升检测的准确度，而对于one-stage方法中，直接采用RPN网络做出精确的预测，因此要在网络设计上做很多的tricks。YOLOv2的一大创新是采用multi-scale training策略，这样同一个模型其实可以使用多种大小的图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv2训练\n",
    "YOLOv2的训练主要是包括三个阶段。第一个阶段就是先在ImageNet分类数据集上预训练DarkNet-19,此时模型输入为224x224,共训练160个epoch，然后第二阶段将网络的输入调整为448x448，继续在ImageNet数据集上finetune分类模型,训练10个epochs,此时分类的模型的top-1准确度为76.5%, 而top-5准确度为93.3%, 第三个阶段就是修改DarkNet-19分类模型为检测模型，并在检测数据集上继续finetune网络.\n",
    "<center> ![training](./pic/training.png)</center>\n",
    "\n",
    "YOLOv2结构示意图：\n",
    "<center> ![arc](./pic/YOLOv2arct.png) </center>\n",
    "\n",
    "论文作者没有明确给出YOLOv2的loss函数的形式，不过使用YOLOv1的loss形式也是可以的，与YOLOv1一样，对于训练图片中的ground truth, 若其中心点落在某个网格中，那么该网格内的5个先验框所对应的边界框负责预测它，具体是由那个与ground truth的IOU最大的边界框预测它。YOLOv2同样需要假定每个网格至多含有一个ground Truth，而在实际上基本不会出现多于1个的情况，与gound Truth匹配的先验框计算坐标误差，置信度度误差以及分类误差，而其他的边界框只计算置信度误差，YOLOv2和YOLOv1的损失函数一样，为均方差函数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
