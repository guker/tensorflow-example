{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES=['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "           'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
    "           'train', 'tvmonitor']\n",
    "\n",
    "PASCAL_PATH = os.path.join('../', 'dataset')\n",
    "CACHE_PATH = os.path.join(PASCAL_PATH, 'cache')\n",
    "OUTPUT_DIR = os.path.join(PASCAL_PATH,'output')\n",
    "WEIGHTS_DIR = os.path.join(PASCAL_PATH, 'weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "class YOLONet(object):\n",
    "    \n",
    "    def __init__(self, is_training=True):\n",
    "        self.classes=CLASSES\n",
    "        self.num_class = len(self.classes)\n",
    "        self.image_size=448\n",
    "        self.cell_size=7\n",
    "        self.boxes_per_cell=2\n",
    "        self.output_size=(self.cell_size*self.cell_size)*\\\n",
    "                    (self.num_class+self.boxes_per_cell*5)\n",
    "        self.scale=1.0*self.image_size/self.cell_size\n",
    "        self.boundary1 = self.cell_size*self.cell_size*self.num_class\n",
    "        self.boundary2 = self.boundary1 + self.cell_size*self.cell_size*self.boxes_per_cell\n",
    "\n",
    "        self.object_scale = 1.0\n",
    "        self.noobject_scale=0.5\n",
    "        self.class_scale=1.0\n",
    "        self.coord_scale=5.0\n",
    "        \n",
    "        self.batch_size=45\n",
    "        self.alpha=0.1\n",
    "        self.offset = np.transpose(np.reshape(\n",
    "            np.array([np.arange(self.cell_size)]*self.cell_size*self.boxes_per_cell),\n",
    "            (self.boxes_per_cell, self.cell_size, self.cell_size)), (1,2,0))\n",
    "        \n",
    "        self.images = tf.placeholder(tf.float32, [None, self.image_size, self.image_size,3],name='images')\n",
    "        self.logits = self.build_network(self.images, num_outputs=self.output_size, alpha=self.alpha,\n",
    "                                        is_training=is_training)\n",
    "        \n",
    "        if is_training:\n",
    "            self.labels = tf.placeholder(tf.float32,[None, self.cell_size,self.cell_size, 5+self.num_class])\n",
    "            self.loss_layer(self.logits, self.labels)\n",
    "            self.total_loss=tf.losses.get_total_loss()\n",
    "            \n",
    "            \n",
    "    def build_network(self, images, num_outputs, alpha,\n",
    "                     keep_prob=0.5, is_training=True,scope='yolo'):\n",
    "        with tf.variable_scope(scope):\n",
    "            with slim.arg_scope(\n",
    "                [slim.conv2d, slim.fully_connected],\n",
    "                activation_fn=leaky_relu(alpha),\n",
    "                weights_regularizer=slim.l2_regularizer(0.0005),\n",
    "                weights_initializer=tf.glorot_uniform_initializer()\n",
    "                #weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)\n",
    "            ):\n",
    "                net = tf.pad(\n",
    "                    images, np.array([[0, 0], [3, 3], [3, 3], [0, 0]]),\n",
    "                    name='pad_1')\n",
    "                net = slim.conv2d(\n",
    "                    net, 64, 7, 2, padding='VALID', scope='conv_2')\n",
    "                net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_3')\n",
    "                net = slim.conv2d(net, 192, 3, scope='conv_4')\n",
    "                net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_5')\n",
    "                net = slim.conv2d(net, 128, 1, scope='conv_6')\n",
    "                net = slim.conv2d(net, 256, 3, scope='conv_7')\n",
    "                net = slim.conv2d(net, 256, 1, scope='conv_8')\n",
    "                net = slim.conv2d(net, 512, 3, scope='conv_9')\n",
    "                net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_10')\n",
    "                net = slim.conv2d(net, 256, 1, scope='conv_11')\n",
    "                net = slim.conv2d(net, 512, 3, scope='conv_12')\n",
    "                net = slim.conv2d(net, 256, 1, scope='conv_13')\n",
    "                net = slim.conv2d(net, 512, 3, scope='conv_14')\n",
    "                net = slim.conv2d(net, 256, 1, scope='conv_15')\n",
    "                net = slim.conv2d(net, 512, 3, scope='conv_16')\n",
    "                net = slim.conv2d(net, 256, 1, scope='conv_17')\n",
    "                net = slim.conv2d(net, 512, 3, scope='conv_18')\n",
    "                net = slim.conv2d(net, 512, 1, scope='conv_19')\n",
    "                net = slim.conv2d(net, 1024, 3, scope='conv_20')\n",
    "                net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_21')\n",
    "                net = slim.conv2d(net, 512, 1, scope='conv_22')\n",
    "                net = slim.conv2d(net, 1024, 3, scope='conv_23')\n",
    "                net = slim.conv2d(net, 512, 1, scope='conv_24')\n",
    "                net = slim.conv2d(net, 1024, 3, scope='conv_25')\n",
    "                net = slim.conv2d(net, 1024, 3, scope='conv_26')\n",
    "                net = tf.pad(\n",
    "                    net, np.array([[0, 0], [1, 1], [1, 1], [0, 0]]),\n",
    "                    name='pad_27')\n",
    "                net = slim.conv2d(\n",
    "                    net, 1024, 3, 2, padding='VALID', scope='conv_28')\n",
    "                net = slim.conv2d(net, 1024, 3, scope='conv_29')\n",
    "                net = slim.conv2d(net, 1024, 3, scope='conv_30')\n",
    "                net = tf.transpose(net, [0, 3, 1, 2], name='trans_31')\n",
    "                net = slim.flatten(net, scope='flat_32')\n",
    "                net = slim.fully_connected(net, 512, scope='fc_33')\n",
    "                net = slim.fully_connected(net, 4096, scope='fc_34')\n",
    "                net = slim.dropout(\n",
    "                    net, keep_prob=keep_prob, is_training=is_training,\n",
    "                    scope='dropout_35')\n",
    "                net = slim.fully_connected(\n",
    "                    net, num_outputs, activation_fn=None, scope='fc_36')\n",
    "        return net\n",
    "    \n",
    "    def calc_iou(self, boxes1, boxes2, scope='iou'):\n",
    "        \"\"\"calculate ious\n",
    "        Args:\n",
    "          boxes1: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "          boxes2: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4] ===> (x_center, y_center, w, h)\n",
    "        Return:\n",
    "          iou: 4-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(scope):\n",
    "            # transform (x_center, y_center, w, h) to (x1, y1, x2, y2)\n",
    "            boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2]/2.0,\n",
    "                                 boxes1[..., 1] - boxes1[..., 3]/2.0,\n",
    "                                 boxes1[..., 0] + boxes1[..., 2]/2.0,\n",
    "                                 boxes1[..., 1] + boxes1[..., 3]/2.0],\n",
    "                                 axis=-1)\n",
    "            \n",
    "            boxes2_t =tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
    "                                boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
    "                                boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
    "                                boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
    "                                axis=-1)\n",
    "            # calculate the left up point & right down point\n",
    "            lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
    "            rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
    "            \n",
    "            # intersection area\n",
    "            intersection = tf.maximum(0., rd - lu)\n",
    "            inter_area = intersection[..., 0]*intersection[..., 1]\n",
    "            \n",
    "            # calculate the boxes1 area and boxes2 area\n",
    "            area1 = boxes1[..., 2]*boxes1[...,3]\n",
    "            area2 = boxes2[..., 2]*boxes2[...,3]\n",
    "            \n",
    "            union_area = tf.maximum(area1 + area2 - inter_area, 1e-10)\n",
    "            return tf.clip_by_value(inter_area/union_area,0.0,1.0)\n",
    "        \n",
    "    def loss_layer(self, predicts, labels, scope='loss_layer'):\n",
    "        '''\n",
    "        predicts: shape (None,1470)\n",
    "        lables: shape (None,7,7,25)\n",
    "        '''\n",
    "        with tf.variable_scope(scope):\n",
    "            # 将网络输出分离为类别和定位以及box大小，输出为维度为7*7*20+7*7*2+7*7*2*4=1470\n",
    "            # 预测类别，形状为(None,7,7,20)\n",
    "            predict_classes = tf.reshape(predicts[:,:self.boundary1],\n",
    "                                      [self.batch_size, self.cell_size, self.cell_size, self.num_class])\n",
    "\n",
    "            # 预测是否有物体，形状为(None,7,7,2)\n",
    "            predict_scales = tf.reshape(predicts[:,self.boundary1:self.boundary2],\n",
    "                                       [self.batch_size, self.cell_size, self.cell_size, self.boxes_per_cell])\n",
    "\n",
    "            # 预测box大小,shape为(None,7,7,2,4)\n",
    "            predict_boxes = tf.reshape(predicts[:,self.boundary2:],\n",
    "                                      [self.batch_size, self.cell_size, self.cell_size, self.boxes_per_cell,4])\n",
    "\n",
    "            # label的是否有目标信息，shape为(None,7,7,1)\n",
    "            response = tf.reshape(labels[...,0],\n",
    "                                 [self.batch_size, self.cell_size, self.cell_size, 1])\n",
    "\n",
    "            # label的定位信息，shape为(None,7,7,1,4)\n",
    "            boxes = tf.reshape(labels[..., 1:5],\n",
    "                              [self.batch_size, self.cell_size, self.cell_size,1,4])\n",
    "\n",
    "            # lable的box大小信息,shape为(None,7,7,2,4)\n",
    "            boxes = tf.tile(boxes,[1,1,1, self.boxes_per_cell,1])/self.image_size\n",
    "\n",
    "            # label的类别信息,shape(None,7,7,20), one_hot\n",
    "            classes = labels[..., 5:]\n",
    "\n",
    "            # offset的shape(1,7,7,2)\n",
    "            offset = tf.reshape(tf.constant(self.offset, dtype=tf.float32),\n",
    "                               [1, self.cell_size, self.cell_size, self.boxes_per_cell])\n",
    "            # 复制batchsize次，shape(batch_size,7,7,2)\n",
    "            offset = tf.tile(offset, [self.batch_size,1,1,1])\n",
    "            offset_tran = tf.transpose(offset, (0,2,1,3))\n",
    "            predict_boxes_tran = tf.stack(\n",
    "                            [(predict_boxes[...,0] + offset)/self.cell_size,\n",
    "                             (predict_boxes[...,1] + offset_tran)/self.cell_size,\n",
    "                              tf.square(predict_boxes[..., 2]),\n",
    "                              tf.square(predict_boxes[..., 3])], axis=-1)\n",
    "            # shape(None,7,7,2)\n",
    "            iou_predict_truth = self.calc_iou(predict_boxes_tran, boxes)\n",
    "            # calculate I tensror [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "            object_mask = tf.reduce_max(iou_predict_truth, 3, keep_dims=True)\n",
    "            object_mask = tf.cast((iou_predict_truth >= object_mask),tf.float32)*response\n",
    "\n",
    "            # calculate no_I tensor [CELL_SIZE,CELL_SIZE,BOXES_PER_CELL]\n",
    "            noobject_mask = tf.ones_like(object_mask, dtype=tf.float32) - object_mask\n",
    "\n",
    "            boxes_tran = tf.stack([boxes[..., 0]*self.cell_size - offset,\n",
    "                                   boxes[..., 1]*self.cell_size - offset_tran,\n",
    "                                   tf.sqrt(boxes[...,2]),\n",
    "                                   tf.sqrt(boxes[...,3])], axis=-1)\n",
    "\n",
    "            # class loss\n",
    "            class_delta = response*(predict_classes - classes)\n",
    "            class_loss = tf.reduce_mean(tf.reduce_sum(tf.square(class_delta), axis=[1,2,3]),\n",
    "                                       name='class_loss')*self.class_scale\n",
    "            \n",
    "            # object_loss\n",
    "            object_delta = object_mask * (predict_scales - iou_predict_truth)\n",
    "            object_loss = tf.reduce_mean(tf.reduce_sum(tf.square(object_delta), axis=[1, 2, 3]),\n",
    "                                       name='object_loss') * self.object_scale\n",
    "            # noobject loss\n",
    "            noobject_delta = noobject_mask*predict_scales\n",
    "            noobject_loss = tf.reduce_mean(\n",
    "                            tf.reduce_sum(tf.square(noobject_delta), axis=[1,2,3]),\n",
    "                            name='noobject_loss')*self.noobject_scale\n",
    "            # coord_loss\n",
    "            coord_mask = tf.expand_dims(object_mask, 4)\n",
    "            boxes_delta = coord_mask*(predict_boxes - boxes_tran)\n",
    "            coord_loss = tf.reduce_mean(\n",
    "                            tf.reduce_sum(tf.square(boxes_delta), axis=[1,2,3,4]),\n",
    "                            name='coord_loss')*self.coord_scale\n",
    "\n",
    "            tf.losses.add_loss(class_loss)\n",
    "            tf.losses.add_loss(object_loss)\n",
    "            tf.losses.add_loss(noobject_loss)\n",
    "            tf.losses.add_loss(coord_loss)\n",
    "            \n",
    "def leaky_relu(alpha):\n",
    "    def op(inputs):\n",
    "        return tf.nn.leaky_relu(inputs, alpha=alpha,name='leaky_relu')\n",
    "    return op   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pascal_voc(object):\n",
    "    def __init__(self, phase, rebuild=False):\n",
    "        self.devkit_path = os.path.join(PASCAL_PATH, 'VOCdevkit')\n",
    "        self.data_path = os.path.join(self.devkit_path,'VOC2007')\n",
    "        self.cache_path = CACHE_PATH\n",
    "        self.batch_size = 45\n",
    "        self.image_size = 448\n",
    "        self.cell_size = 7\n",
    "        self.classes = CLASSES\n",
    "        self.class_to_ind = dict(zip(self.classes, range(len(self.classes))))\n",
    "        self.flipped = True\n",
    "        self.phase = phase\n",
    "        self.rebuild = rebuild\n",
    "        self.cursor = 0\n",
    "        self.epoch = 1\n",
    "        self.prepare()\n",
    "    \n",
    "    def get(self):\n",
    "        images = np.zeros((self.batch_size, self.image_size, self.image_size, 3))\n",
    "        labels = np.zeros((self.batch_size, self.cell_size, self.cell_size,25))\n",
    "        \n",
    "        count = 0\n",
    "        while count < self.batch_size:\n",
    "            imname = self.gt_labels[self.cursor]['imname']\n",
    "            flipped = self.gt_labels[self.cursor]['flipped']\n",
    "            images[count,:,:,:] = self.image_read(imname,flipped)\n",
    "            labels[count,:,:,:] = self.gt_labels[self.cursor]['label']\n",
    "            count += 1\n",
    "            self.cursor += 1\n",
    "            if self.cursor >= len(self.gt_labels):\n",
    "                np.random.shuffle(self.gt_labels)\n",
    "                self.cursor = 0\n",
    "                self.epoch += 1\n",
    "        return images, labels\n",
    "    \n",
    "    def image_read(self, imname, flipped = False):\n",
    "        image = cv2.imread(imname)\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image = (image/255.0)*2.0 - 1\n",
    "        if flipped:\n",
    "            image = image[:,::-1,:]\n",
    "        return image\n",
    "    \n",
    "    def prepare(self):\n",
    "        gt_labels = self.load_labels()\n",
    "        if self.flipped:\n",
    "            print('Appending horizontally-flipped training examples ...')\n",
    "            gt_labels_cp = copy.deepcopy(gt_labels)\n",
    "            for idx in range(len(gt_labels_cp)):\n",
    "                gt_labels_cp[idx]['flipped'] = True\n",
    "                gt_labels_cp[idx]['label'] = gt_labels_cp[idx]['label'][:,::-1,:]\n",
    "                \n",
    "                for i in range(self.cell_size):\n",
    "                    for j in range(self.cell_size):\n",
    "                        if gt_labels_cp[idx]['label'][i,j,0] == 1:\n",
    "                            # box的(x,y,w,h)中只有x需要变换\n",
    "                            gt_labels_cp[idx]['label'][i,j,1] = self.image_size - 1 - gt_labels_cp[idx]['label'][i,j,1]\n",
    "            # 翻转后的label直接拼接在原label的后面                \n",
    "            gt_labels += gt_labels_cp \n",
    "        np.random.shuffle(gt_labels)\n",
    "        self.gt_labels = gt_labels\n",
    "        return gt_labels\n",
    "    \n",
    "    def load_labels(self):\n",
    "        cache_file = os.path.join(\n",
    "                    self.cache_path, 'pascal_' + self.phase + '_gt_labels.pkl')\n",
    "        \n",
    "        if os.path.isfile(cache_file) and not self.rebuild:\n",
    "            print('Loading gt_labels from:' + cache_file)\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                gt_labels = pickle.load(f)\n",
    "            return gt_labels\n",
    "        \n",
    "        print('Processing gt_labels from: ' + self.data_path)\n",
    "        \n",
    "        if not os.path.exists(self.cache_path):\n",
    "            os.makedirs(self.cache_path)\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            txtname = os.path.join(self.data_path,'ImageSets','Main','trainval.txt')\n",
    "        else:\n",
    "            txtname = os.path.join(self.data_path,'ImageSets','Main','test.txt')\n",
    "        with open(txtname, 'r') as f:\n",
    "            self.image_index = [x.strip() for x in f.readlines()]\n",
    "        \n",
    "        gt_labels = []\n",
    "        for index in self.image_index:\n",
    "            label,num = self.load_pascal_annotation(index)\n",
    "            if num == 0:\n",
    "                continue\n",
    "            imname = os.path.join(self.data_path,'JPEGImages', index + '.jpg')\n",
    "            gt_labels.append({'imname': imname,\n",
    "                              'label': label,\n",
    "                              'flipped': False})\n",
    "        print('Saving gt_labels to: ' + cache_file)\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(gt_labels, f)\n",
    "        return gt_labels\n",
    "    \n",
    "    def load_pascal_annotation(self, index):\n",
    "        '''\n",
    "        Load image and bounding boxes info from XML file in the PASCAL VOC format.\n",
    "        '''\n",
    "        imname = os.path.join(self.data_path,'JPEGImages', index+'.jpg')\n",
    "        im = cv2.imread(imname)\n",
    "        h_ratio = 1.0*self.image_size/im.shape[0]\n",
    "        w_ratio = 1.0*self.image_size/im.shape[1]\n",
    "        \n",
    "        label = np.zeros((self.cell_size,self.cell_size,25))\n",
    "        filename = os.path.join(self.data_path, 'Annotations',index+'.xml')\n",
    "        tree = ET.parse(filename)\n",
    "        objs = tree.findall('object')\n",
    "        \n",
    "        for obj in objs:\n",
    "            bbox = obj.find('bndbox')\n",
    "            x1 = max(min((float(bbox.find('xmin').text) - 1) * w_ratio, self.image_size - 1), 0)\n",
    "            y1 = max(min((float(bbox.find('ymin').text) - 1) * h_ratio, self.image_size - 1), 0)\n",
    "            x2 = max(min((float(bbox.find('xmax').text) - 1) * w_ratio, self.image_size - 1), 0)\n",
    "            y2 = max(min((float(bbox.find('ymax').text) - 1) * h_ratio, self.image_size - 1), 0)\n",
    "            cls_ind = self.class_to_ind[obj.find('name').text.lower().strip()]\n",
    "            # boxes为(x,y,w,h)，此时尺寸是在448x448上\n",
    "            boxes = [(x2 + x1)/2.0, (y2 + y1)/2.0, x2 - x1, y2 - y1]\n",
    "            # grid cell的index\n",
    "            x_ind = int(boxes[0] * self.cell_size / self.image_size)\n",
    "            y_ind = int(boxes[1] * self.cell_size / self.image_size)\n",
    "            if label[y_ind, x_ind, 0] == 1:\n",
    "                continue\n",
    "            # label中对应的cell有物体\n",
    "            label[y_ind, x_ind, 0] = 1\n",
    "            # label中对应的cell中物体的bbox\n",
    "            label[y_ind, x_ind, 1:5] = boxes\n",
    "            # label中对应的cell中物体的类别，one-hot\n",
    "            label[y_ind, x_ind, 5 + cls_ind] = 1\n",
    "        return label,len(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b6a73b46baab>:177: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Loading gt_labels from:../dataset/cache/pascal_train_gt_labels.pkl\n",
      "Appending horizontally-flipped training examples ...\n",
      "Restoring weights from: ./YOLO_small.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./YOLO_small.ckpt\n",
      "step: 100, loss: 4.264373779296875\n",
      "step: 200, loss: 4.742156982421875\n",
      "step: 300, loss: 4.636838436126709\n",
      "step: 400, loss: 4.487580299377441\n",
      "step: 500, loss: 5.612963676452637\n",
      "step: 600, loss: 4.921511650085449\n",
      "step: 700, loss: 4.739345550537109\n",
      "step: 800, loss: 4.191561698913574\n",
      "step: 900, loss: 4.3981218338012695\n",
      "step: 1000, loss: 4.486600875854492\n",
      "step: 1100, loss: 4.5085954666137695\n",
      "step: 1200, loss: 4.81608247756958\n",
      "step: 1300, loss: 4.395411491394043\n",
      "step: 1400, loss: 4.146206855773926\n",
      "step: 1500, loss: 3.974743366241455\n",
      "step: 1600, loss: 4.32240104675293\n",
      "step: 1700, loss: 5.350527286529541\n",
      "step: 1800, loss: 5.222931861877441\n",
      "step: 1900, loss: 4.13148307800293\n",
      "step: 2000, loss: 4.863045692443848\n",
      "step: 2100, loss: 3.938716173171997\n",
      "step: 2200, loss: 4.545066833496094\n",
      "step: 2300, loss: 4.060568332672119\n",
      "step: 2400, loss: 4.151005744934082\n",
      "step: 2500, loss: 4.718411445617676\n",
      "step: 2600, loss: 4.0513105392456055\n",
      "step: 2700, loss: 4.996946334838867\n",
      "step: 2800, loss: 4.45883846282959\n",
      "step: 2900, loss: 4.48720645904541\n",
      "step: 3000, loss: 4.629833698272705\n",
      "step: 3100, loss: 4.457071304321289\n",
      "step: 3200, loss: 3.9904096126556396\n",
      "step: 3300, loss: 4.275957107543945\n",
      "step: 3400, loss: 4.13329553604126\n",
      "step: 3500, loss: 4.770547866821289\n",
      "step: 3600, loss: 3.938627243041992\n",
      "step: 3700, loss: 3.9564831256866455\n",
      "step: 3800, loss: 4.518697738647461\n",
      "step: 3900, loss: 3.9353158473968506\n",
      "step: 4000, loss: 4.327796936035156\n",
      "step: 4100, loss: 4.52482795715332\n",
      "step: 4200, loss: 4.501148700714111\n",
      "step: 4300, loss: 4.481197357177734\n",
      "step: 4400, loss: 4.276609420776367\n",
      "step: 4500, loss: 4.230535984039307\n",
      "step: 4600, loss: 4.381619453430176\n",
      "step: 4700, loss: 4.897637367248535\n",
      "step: 4800, loss: 3.8290305137634277\n",
      "step: 4900, loss: 4.112558364868164\n",
      "step: 5000, loss: 4.008487224578857\n",
      "step: 5100, loss: 3.8859760761260986\n",
      "step: 5200, loss: 4.411874771118164\n",
      "step: 5300, loss: 4.177549362182617\n",
      "step: 5400, loss: 3.9856302738189697\n",
      "step: 5500, loss: 3.9069342613220215\n",
      "step: 5600, loss: 3.797682762145996\n",
      "step: 5700, loss: 4.349328994750977\n",
      "step: 5800, loss: 3.7420876026153564\n",
      "step: 5900, loss: 4.45847225189209\n",
      "step: 6000, loss: 3.941866874694824\n",
      "step: 6100, loss: 4.031219959259033\n",
      "step: 6200, loss: 3.653250217437744\n",
      "step: 6300, loss: 4.346148490905762\n",
      "step: 6400, loss: 3.961564779281616\n",
      "step: 6500, loss: 4.180777549743652\n",
      "step: 6600, loss: 4.043426513671875\n",
      "step: 6700, loss: 4.054706573486328\n",
      "step: 6800, loss: 3.913778781890869\n",
      "step: 6900, loss: 3.86775803565979\n",
      "step: 7000, loss: 3.735395908355713\n",
      "step: 7100, loss: 3.5886073112487793\n",
      "step: 7200, loss: 4.123189926147461\n",
      "step: 7300, loss: 3.965707302093506\n",
      "step: 7400, loss: 4.24050235748291\n",
      "step: 7500, loss: 4.189974784851074\n",
      "step: 7600, loss: 4.0421600341796875\n",
      "step: 7700, loss: 3.703862190246582\n",
      "step: 7800, loss: 3.9081954956054688\n",
      "step: 7900, loss: 3.8376893997192383\n",
      "step: 8000, loss: 4.023758888244629\n",
      "step: 8100, loss: 3.6306910514831543\n",
      "step: 8200, loss: 3.7550740242004395\n",
      "step: 8300, loss: 4.220527172088623\n",
      "step: 8400, loss: 3.9895052909851074\n",
      "step: 8500, loss: 3.709482192993164\n",
      "step: 8600, loss: 4.207707405090332\n",
      "step: 8700, loss: 3.65657639503479\n",
      "step: 8800, loss: 3.8877816200256348\n",
      "step: 8900, loss: 4.007416725158691\n",
      "step: 9000, loss: 3.9605891704559326\n",
      "step: 9100, loss: 3.8280391693115234\n",
      "step: 9200, loss: 4.151068687438965\n",
      "step: 9300, loss: 3.742053270339966\n",
      "step: 9400, loss: 3.829050064086914\n",
      "step: 9500, loss: 4.119292259216309\n",
      "step: 9600, loss: 4.195377349853516\n",
      "step: 9700, loss: 3.8582961559295654\n",
      "step: 9800, loss: 4.464292526245117\n",
      "step: 9900, loss: 3.6289286613464355\n",
      "step: 10000, loss: 4.178613662719727\n",
      "INFO:tensorflow:./yolo10000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "step: 10100, loss: 4.457102298736572\n",
      "step: 10200, loss: 4.292304515838623\n",
      "step: 10300, loss: 4.109906196594238\n",
      "step: 10400, loss: 3.725113868713379\n",
      "step: 10500, loss: 3.896449327468872\n",
      "step: 10600, loss: 3.690446376800537\n",
      "step: 10700, loss: 3.63785982131958\n",
      "step: 10800, loss: 3.868602752685547\n",
      "step: 10900, loss: 3.6237339973449707\n",
      "step: 11000, loss: 3.6213135719299316\n",
      "step: 11100, loss: 3.680199384689331\n",
      "step: 11200, loss: 4.436964511871338\n",
      "step: 11300, loss: 3.9809327125549316\n",
      "step: 11400, loss: 3.784794807434082\n",
      "step: 11500, loss: 3.560908794403076\n",
      "step: 11600, loss: 3.9862897396087646\n",
      "step: 11700, loss: 3.797224283218384\n",
      "step: 11800, loss: 4.119974136352539\n",
      "step: 11900, loss: 4.081520080566406\n",
      "step: 12000, loss: 3.9434285163879395\n",
      "step: 12100, loss: 3.974332332611084\n",
      "step: 12200, loss: 3.608307361602783\n",
      "step: 12300, loss: 3.920806884765625\n",
      "step: 12400, loss: 4.07169771194458\n",
      "step: 12500, loss: 3.547300100326538\n",
      "step: 12600, loss: 3.713796615600586\n",
      "step: 12700, loss: 3.8524532318115234\n",
      "step: 12800, loss: 3.7713582515716553\n",
      "step: 12900, loss: 3.9461917877197266\n",
      "step: 13000, loss: 3.7552034854888916\n",
      "step: 13100, loss: 3.7644214630126953\n",
      "step: 13200, loss: 4.182706356048584\n",
      "step: 13300, loss: 3.820463180541992\n",
      "step: 13400, loss: 3.7815299034118652\n",
      "step: 13500, loss: 4.38563871383667\n",
      "step: 13600, loss: 4.0192413330078125\n",
      "step: 13700, loss: 3.99364972114563\n",
      "step: 13800, loss: 3.7175142765045166\n",
      "step: 13900, loss: 3.859030246734619\n",
      "step: 14000, loss: 3.8957560062408447\n",
      "step: 14100, loss: 3.540778636932373\n",
      "step: 14200, loss: 3.532174825668335\n",
      "step: 14300, loss: 3.7260403633117676\n",
      "step: 14400, loss: 3.8407063484191895\n",
      "step: 14500, loss: 3.865138530731201\n",
      "step: 14600, loss: 3.7186660766601562\n",
      "step: 14700, loss: 4.045855522155762\n",
      "step: 14800, loss: 3.681001663208008\n",
      "step: 14900, loss: 4.274532318115234\n",
      "step: 15000, loss: 3.9056692123413086\n",
      "step: 15100, loss: 3.8859734535217285\n",
      "step: 15200, loss: 3.878202438354492\n",
      "step: 15300, loss: 3.978224515914917\n",
      "step: 15400, loss: 3.9473743438720703\n",
      "step: 15500, loss: 3.542654275894165\n",
      "step: 15600, loss: 3.6141600608825684\n",
      "step: 15700, loss: 3.757014751434326\n",
      "step: 15800, loss: 4.006406307220459\n",
      "step: 15900, loss: 3.9335224628448486\n",
      "step: 16000, loss: 3.7112903594970703\n",
      "step: 16100, loss: 4.147579193115234\n",
      "step: 16200, loss: 3.855492115020752\n",
      "step: 16300, loss: 4.046721458435059\n",
      "step: 16400, loss: 4.128782749176025\n",
      "step: 16500, loss: 3.6754050254821777\n",
      "step: 16600, loss: 3.6456754207611084\n",
      "step: 16700, loss: 3.971797466278076\n",
      "step: 16800, loss: 3.611969470977783\n",
      "step: 16900, loss: 3.8790833950042725\n",
      "step: 17000, loss: 4.211759567260742\n",
      "step: 17100, loss: 3.8252220153808594\n",
      "step: 17200, loss: 3.865190029144287\n",
      "step: 17300, loss: 3.7217516899108887\n",
      "step: 17400, loss: 3.8358871936798096\n",
      "step: 17500, loss: 3.697237491607666\n",
      "step: 17600, loss: 3.7781124114990234\n",
      "step: 17700, loss: 3.519420623779297\n",
      "step: 17800, loss: 3.918452739715576\n",
      "step: 17900, loss: 3.771955728530884\n",
      "step: 18000, loss: 3.521392583847046\n",
      "step: 18100, loss: 3.703608989715576\n",
      "step: 18200, loss: 3.8630616664886475\n",
      "step: 18300, loss: 3.8720250129699707\n",
      "step: 18400, loss: 3.6459901332855225\n",
      "step: 18500, loss: 3.6653237342834473\n",
      "step: 18600, loss: 3.5071768760681152\n",
      "step: 18700, loss: 4.383179664611816\n",
      "step: 18800, loss: 3.5889675617218018\n",
      "step: 18900, loss: 3.956644296646118\n",
      "step: 19000, loss: 3.446071147918701\n",
      "step: 19100, loss: 4.070774078369141\n",
      "step: 19200, loss: 3.7615113258361816\n",
      "step: 19300, loss: 3.8954315185546875\n",
      "step: 19400, loss: 3.939972162246704\n",
      "step: 19500, loss: 3.636955738067627\n",
      "step: 19600, loss: 3.781365394592285\n",
      "step: 19700, loss: 3.6650869846343994\n",
      "step: 19800, loss: 3.8821730613708496\n",
      "step: 19900, loss: 4.030124187469482\n",
      "step: 20000, loss: 4.092226028442383\n",
      "INFO:tensorflow:./yolo20000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "step: 20100, loss: 3.961878776550293\n",
      "step: 20200, loss: 3.604097843170166\n",
      "step: 20300, loss: 3.8143954277038574\n",
      "step: 20400, loss: 3.8097877502441406\n",
      "step: 20500, loss: 3.865591049194336\n",
      "step: 20600, loss: 3.8070433139801025\n",
      "step: 20700, loss: 4.096120834350586\n",
      "step: 20800, loss: 4.002724647521973\n",
      "step: 20900, loss: 4.074027061462402\n",
      "step: 21000, loss: 3.5557966232299805\n",
      "step: 21100, loss: 4.340010643005371\n",
      "step: 21200, loss: 4.145789623260498\n",
      "step: 21300, loss: 3.73297119140625\n",
      "step: 21400, loss: 4.219810962677002\n",
      "step: 21500, loss: 3.5947012901306152\n",
      "step: 21600, loss: 3.975776195526123\n",
      "step: 21700, loss: 3.889984607696533\n",
      "step: 21800, loss: 3.745824098587036\n",
      "step: 21900, loss: 3.8328375816345215\n",
      "step: 22000, loss: 3.902529716491699\n",
      "step: 22100, loss: 3.893134355545044\n",
      "step: 22200, loss: 3.658076286315918\n",
      "step: 22300, loss: 3.997262954711914\n",
      "step: 22400, loss: 3.4862887859344482\n",
      "step: 22500, loss: 3.867987632751465\n",
      "step: 22600, loss: 4.118494033813477\n",
      "step: 22700, loss: 3.7497549057006836\n",
      "step: 22800, loss: 4.25978946685791\n",
      "step: 22900, loss: 3.567314386367798\n",
      "step: 23000, loss: 4.191647529602051\n",
      "step: 23100, loss: 3.6684954166412354\n",
      "step: 23200, loss: 3.6948156356811523\n",
      "step: 23300, loss: 4.053374290466309\n",
      "step: 23400, loss: 4.0778398513793945\n",
      "step: 23500, loss: 4.127681732177734\n",
      "step: 23600, loss: 3.919649839401245\n",
      "step: 23700, loss: 3.5249037742614746\n",
      "step: 23800, loss: 4.028938293457031\n",
      "step: 23900, loss: 3.546309471130371\n",
      "step: 24000, loss: 3.72113037109375\n",
      "step: 24100, loss: 3.7591733932495117\n",
      "step: 24200, loss: 3.7079811096191406\n",
      "step: 24300, loss: 3.6086745262145996\n",
      "step: 24400, loss: 3.9727320671081543\n",
      "step: 24500, loss: 3.916508674621582\n",
      "step: 24600, loss: 3.8701155185699463\n",
      "step: 24700, loss: 3.6325149536132812\n",
      "step: 24800, loss: 3.8322360515594482\n",
      "step: 24900, loss: 4.101827621459961\n",
      "step: 25000, loss: 3.534053325653076\n",
      "INFO:tensorflow:./yolo25000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, net, data):\n",
    "        self.net = net\n",
    "        self.data = data\n",
    "        self.max_iter = 25000\n",
    "        self.initial_learning_rate = 0.001\n",
    "        self.decay_steps = 10000\n",
    "        self.decay_rate = 0.1\n",
    "        self.staircase = True\n",
    "        self.weights_file = './YOLO_small.ckpt'\n",
    "        \n",
    "        \n",
    "        self.variable_to_restore = tf.global_variables()\n",
    "        self.saver = tf.train.Saver(self.variable_to_restore, max_to_keep=None)\n",
    "        self.global_step = tf.train.create_global_step()\n",
    "        self.learning_rate = tf.train.exponential_decay(\n",
    "                            self.initial_learning_rate,self.global_step,\n",
    "                            self.decay_steps, self.decay_rate, self.staircase,\n",
    "                            name='learning_rate')\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(\n",
    "                            learning_rate=self.learning_rate)\n",
    "        self.train_op = slim.learning.create_train_op(\n",
    "                            self.net.total_loss, self.optimizer,global_step=self.global_step)\n",
    "        \n",
    "        gpu_options = tf.GPUOptions()\n",
    "        config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        if self.weights_file is not None:\n",
    "            print('Restoring weights from: ' + self.weights_file)\n",
    "            self.saver.restore(self.sess, self.weights_file)\n",
    "            \n",
    "    def train(self):\n",
    "        for step in range(1,self.max_iter + 1):\n",
    "            images,labels = self.data.get()\n",
    "            feed_dict = {self.net.images: images,\n",
    "                         self.net.labels: labels}\n",
    "            \n",
    "            loss,_ = self.sess.run([self.net.total_loss, self.train_op],\n",
    "                                  feed_dict=feed_dict)\n",
    "            if step % 100 == 0:\n",
    "                print('step: {}, loss: {}'.format(step, loss))\n",
    "                \n",
    "            checkpoint_name = os.path.join('.', 'yolo' + str(step) + '.ckpt')\n",
    "            if step %10000 == 0 and step != 0:\n",
    "                self.saver.save(self.sess, checkpoint_name)\n",
    "        self.saver.save(self.sess, checkpoint_name)\n",
    "yolo = YOLONet()\n",
    "pascal = pascal_voc('train')\n",
    "solver = Solver(yolo, pascal)\n",
    "solver.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./yolo25000.ckpt\n",
      "[['person', 180.51026548658098, 233.84686027254375, 232.38132681165425, 291.361825806754, 0.9480621218681335], ['person', 228.30344949449812, 133.53401848248072, 111.04434728622437, 112.0371605668749, 0.43928977847099304], ['bottle', 390.2549743652344, 226.4765841620309, 35.41623055934906, 86.59440279006958, 0.2403481900691986], ['diningtable', 348.6579486301967, 294.4240399769374, 196.0460628782, 129.76182358605521, 0.23747755587100983], ['person', 42.55091292517526, 198.42875003814697, 44.54611028943743, 209.22584193093437, 0.22830785810947418], ['person', 325.0153745923724, 149.037058864321, 90.14904499053955, 117.4522978918893, 0.21839332580566406]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "class Detector(object):\n",
    "    def __init__(self, net, weight_file):\n",
    "        self.net = net\n",
    "        self.weights_file = weight_file\n",
    "        \n",
    "        self.classes = CLASSES\n",
    "        self.num_class = len(self.classes)\n",
    "        self.image_size = 448\n",
    "        self.cell_size = 7\n",
    "        self.boxes_per_cell = 2\n",
    "        self.threshold = 0.2\n",
    "        self.iou_threshold = 0.5\n",
    "        self.boundary1 = self.cell_size*self.cell_size*self.num_class\n",
    "        self.boundary2 = self.boundary1 + self.cell_size*self.cell_size*self.boxes_per_cell\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.saver.restore(self.sess, self.weights_file)\n",
    "        \n",
    "    def draw_result(self,img,result):\n",
    "        for i in range(len(result)):\n",
    "            x = int(result[i][1])\n",
    "            y = int(result[i][2])\n",
    "            w = int(result[i][3]/2)\n",
    "            h = int(result[i][4]/2)\n",
    "            \n",
    "            cv2.rectangle(img,(x-w,y-h),(x+w,y+h),(0,255,0),2)\n",
    "            cv2.rectangle(img,(x-w,y-h-20),(x+w,y-h),(125,125,125),-1)\n",
    "            lineType = cv2.LINE_AA if cv2.__version__ > '3' else cv2.CV_AA\n",
    "            cv2.putText(\n",
    "                img, result[i][0]+' :%.2f'%result[i][5],\n",
    "                (x-w+5,y-h-7),cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                (0,0,0),1, lineType)\n",
    "            \n",
    "    def detect(self,img):\n",
    "        img_h,img_w,_ = img.shape\n",
    "        inputs = cv2.resize(img, (self.image_size, self.image_size))\n",
    "        inputs = cv2.cvtColor(inputs, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        inputs = (inputs / 255.0) * 2.0 - 1.0\n",
    "        inputs = np.reshape(inputs, (1, self.image_size, self.image_size, 3))\n",
    "        \n",
    "        result = self.detect_from_cvmat(inputs)[0]\n",
    "        \n",
    "        for i in range(len(result)):\n",
    "            result[i][1] *= (1.0 * img_w / self.image_size)\n",
    "            result[i][2] *= (1.0 * img_h / self.image_size)\n",
    "            result[i][3] *= (1.0 * img_w / self.image_size)\n",
    "            result[i][4] *= (1.0 * img_h / self.image_size)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def detect_from_cvmat(self, inputs):\n",
    "        net_output = self.sess.run(self.net.logits,\n",
    "                                  feed_dict={self.net.images: inputs})\n",
    "        results = []\n",
    "        for i in range(net_output.shape[0]):\n",
    "            results.append(self.interpret_output(net_output[i]))\n",
    "        return results\n",
    "    \n",
    "    def interpret_output(self, output):\n",
    "        probs = np.zeros((self.cell_size,self.cell_size,\n",
    "                         self.boxes_per_cell, self.num_class))\n",
    "        class_probs = np.reshape(output[0:self.boundary1],\n",
    "                                (self.cell_size, self.cell_size,self.num_class))\n",
    "        scales = np.reshape(output[self.boundary1:self.boundary2],\n",
    "                           (self.cell_size, self.cell_size, self.boxes_per_cell))\n",
    "        boxes = np.reshape(output[self.boundary2:],\n",
    "                          (self.cell_size, self.cell_size,self.boxes_per_cell,4))\n",
    "        offset = np.array([np.arange(self.cell_size)]*self.cell_size*self.boxes_per_cell)\n",
    "        offset = np.transpose(np.reshape(\n",
    "                              offset, [self.boxes_per_cell,self.cell_size,self.cell_size]),(1,2,0))\n",
    "        \n",
    "        boxes[:,:,:,0] += offset\n",
    "        boxes[:,:,:,1] += np.transpose(offset,(1,0,2))\n",
    "        boxes[:,:,:,:2] = 1.0*boxes[:,:,:,0:2]/self.cell_size\n",
    "        boxes[:,:,:,2:] = np.square(boxes[:,:,:,2:])\n",
    "        \n",
    "        boxes *= self.image_size\n",
    "        \n",
    "        for i in range(self.boxes_per_cell):\n",
    "            for j in range(self.num_class):\n",
    "                probs[:,:,i,j] = np.multiply(class_probs[:,:,j],scales[:,:,i])\n",
    "         \n",
    "        filter_mat_probs = np.array(probs >= self.threshold, dtype='bool')\n",
    "        filter_mat_boxes = np.nonzero(filter_mat_probs)\n",
    "        boxes_filtered = boxes[filter_mat_boxes[0],\n",
    "                               filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "        probs_filtered = probs[filter_mat_probs]\n",
    "        classes_num_filtered = np.argmax(\n",
    "            filter_mat_probs, axis=3)[\n",
    "            filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "\n",
    "        argsort = np.array(np.argsort(probs_filtered))[::-1]\n",
    "        boxes_filtered = boxes_filtered[argsort]\n",
    "        probs_filtered = probs_filtered[argsort]\n",
    "        classes_num_filtered = classes_num_filtered[argsort]\n",
    "\n",
    "        for i in range(len(boxes_filtered)):\n",
    "            if probs_filtered[i] == 0:\n",
    "                continue\n",
    "            for j in range(i + 1, len(boxes_filtered)):\n",
    "                if self.iou(boxes_filtered[i], boxes_filtered[j]) > self.iou_threshold:\n",
    "                    probs_filtered[j] = 0.0\n",
    "\n",
    "        filter_iou = np.array(probs_filtered > 0.0, dtype='bool')\n",
    "        boxes_filtered = boxes_filtered[filter_iou]\n",
    "        probs_filtered = probs_filtered[filter_iou]\n",
    "        classes_num_filtered = classes_num_filtered[filter_iou]\n",
    "\n",
    "        result = []\n",
    "        for i in range(len(boxes_filtered)):\n",
    "            result.append(\n",
    "                [self.classes[classes_num_filtered[i]],\n",
    "                 boxes_filtered[i][0],\n",
    "                 boxes_filtered[i][1],\n",
    "                 boxes_filtered[i][2],\n",
    "                 boxes_filtered[i][3],\n",
    "                 probs_filtered[i]])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def iou(self, box1, box2):\n",
    "        tb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - \\\n",
    "            max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2])\n",
    "        lr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - \\\n",
    "            max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3])\n",
    "        inter = 0 if tb < 0 or lr < 0 else tb * lr\n",
    "        return inter / (box1[2] * box1[3] + box2[2] * box2[3] - inter)\n",
    "\n",
    "    def camera_detector(self, cap, wait=10):\n",
    "        ret, _ = cap.read()\n",
    "\n",
    "        while ret:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            result = self.detect(frame)\n",
    "        \n",
    "            self.draw_result(frame, result)\n",
    "            cv2.imshow('Camera', frame)\n",
    "            cv2.waitKey(wait)\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "    def image_detector(self, imname, wait=0):\n",
    "        \n",
    "        image = cv2.imread(imname)\n",
    "        result = self.detect(image)\n",
    "        print(result)\n",
    "        self.draw_result(image, result)\n",
    "        cv2.imwrite('demo.jpg',image)\n",
    "        #cv2.imshow('Image', image)\n",
    "        #cv2.waitKey(wait)\n",
    "\n",
    "yolo = YOLONet(False)\n",
    "weight_file = './yolo25000.ckpt'\n",
    "#weight_file='./YOLO_small.ckpt'\n",
    "detector = Detector(yolo,weight_file)\n",
    "imname = '../dataset/VOCdevkit/VOC2007/JPEGImages/000050.jpg'\n",
    "detector.image_detector(imname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dection](./demo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
